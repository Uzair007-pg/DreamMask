# Training FC-CLIP on DreamMask-style NSS Synthetic Dataset

This document explains how to train **FC-CLIP** (with DreamMask-style synthetic–real alignment) on the synthetic dataset generated by the NSS pipeline (CNA + layout generation + mask & score-based selection).

It is meant to *extend* the original FC-CLIP README. You can either:
- Add a link to this file from the main `README.md`, or
- Append the relevant sections directly into `README.md`.

---

## 1. Prerequisites

You should already have the following pipeline set up and executed:

1. **Category Name Association (CNA) + Layout Generation**  
   From your custom NSS repo (outside FC-CLIP), you should have:
   - `CNA.py` (COCO-based category name association)
   - `batch_context_layout_generator.py` (batch context-aware layout generation)
   - `mask_and_selection.py` (SAM + CLIP score-based selection)

2. **Layout-to-Image Generation (e.g., LayoutGPT)**  
   After using LayoutGPT (or another layout-to-image model), your intermediate dataset should look like:

   ```text
   layouts_dataset/
     scene_00000.html
     scene_00000.png        # layout visualization
     real_00000.png         # realistic synthesized image
     scene_00000.json       # metadata with bounding boxes
     scene_00001.html
     scene_00001.png
     real_00001.png
     scene_00001.json
     ...
   ```

3. **Mask Generation + Score-based Selection**  

   Run:

   ```bash
   python mask_and_selection.py
   ```

   This will create a **processed** synthetic dataset:

   ```text
   processed_dataset/
     images/
       real_00000.png
       real_00001.png
       ...
     masks/
       scene_00000_obj_00.png
       scene_00000_obj_01.png
       ...
     scene_00000.json
     scene_00001.json
     ...
   ```

Each `scene_XXXXX.json` in `processed_dataset/` should have the format:

```json
{
  "scene_id": "scene_00000",
  "real_image": "images/real_00000.png",
  "canvas_width": 1024,
  "canvas_height": 1024,
  "objects": [
    {
      "index": 0,
      "name": "car",
      "box": { "x": 120, "y": 430, "w": 260, "h": 140 },
      "mask_path": "masks/scene_00000_obj_00.png",
      "clip_score": 0.35,
      "mask_uncertainty": 0.08
    }
  ]
}
```

The **NSS pipeline** you are using (CNA → layout → LayoutGPT → mask & selection) is responsible for creating these JSONs, masks and images.

---

## 2. NSS-Compatible Training Script: `train_net_nss.py`

We provide a modified training script:

- **Original**: `train_net.py` (from FC-CLIP)
- **New**: `train_net_nss.py` (for DreamMask-style NSS data)

Place `train_net_nss.py` in the FC-CLIP repo root (next to the original `train_net.py`).

### 2.1 What `train_net_nss.py` Does

1. **Registers a new Detectron2 dataset**  
   - Name: `nss_processed_train`  
   - Root: `./processed_dataset`  

   It scans all `scene_*.json` files and builds a Detectron2 **instance segmentation** dataset:

   - `file_name` → `processed_dataset/` + `real_image`  
   - `height`, `width` → inferred from the real image  
   - `annotations` → for each object:
     - `bbox` = `[x, y, w, h]`
     - `bbox_mode` = `BoxMode.XYWH_ABS`
     - `category_id` = integer id derived from `name`
     - `segmentation` = COCO-style **RLE** encoded from the binary mask PNG
     - `iscrowd` = 0  

   It also sets:

   ```python
   MetadataCatalog.get("nss_processed_train").set(
       thing_classes=[list_of_category_names],
       evaluator_type="coco",
   )
   ```

2. **Marks samples as synthetic for DreamMask**

   For every sample from `processed_dataset`, `train_net_nss.py` creates a dataset dict like:

   ```python
   record = {
       "file_name": image_path,
       "image_id": img_id,
       "height": height,
       "width": width,
       "is_synthetic": True,   # <--- important for DreamMask SRA
       "annotations": [...],
   }
   ```

   The modified instance & panoptic dataset mappers (`COCOInstanceNewBaselineDatasetMapper` / `COCOPanopticNewBaselineDatasetMapper`) pass this flag through to the model as:

   ```python
   dataset_dict["is_synthetic"] = bool(dataset_dict.get("is_synthetic", False))
   ```

   During training, `FCCLIP.forward` can then read:

   ```python
   is_synthetic = bool(batched_inputs[i].get("is_synthetic", False))
   ```

   to decide whether the batch is **real** or **synthetic**, and apply the synthetic–real alignment loss accordingly.

3. **Overrides dataset in the config**

   In `setup(args)`, the script:

   ```python
   register_nss_processed_dataset("processed_dataset", "nss_processed_train")
   cfg.defrost()
   cfg.DATASETS.TRAIN = ("nss_processed_train",)
   cfg.DATASETS.TEST = ()  # no test set by default
   cfg.INPUT.DATASET_MAPPER_NAME = "mask_former_instance"
   cfg.freeze()
   ```

   This means:

   - Whatever COCO/Cityscapes config you use, the *train set* is replaced with `nss_processed_train`.
   - It uses `MaskFormerInstanceDatasetMapper` for instance segmentation.
   - Test datasets are disabled by default for this synthetic-only setup.

4. **Keeps all FC-CLIP / Mask2Former logic**

   - Optimizer, scheduler, CLIP tower freezing, etc. all remain the same as in the original `train_net.py`.
   - You can still use the original FC-CLIP configs (just add DreamMask fields as described below).

---

## 3. DreamMask Synthetic–Real Alignment in FC-CLIP

The `FCCLIP` meta-architecture has been modified to incorporate the DreamMask-style synthetic–real alignment loss:

\[
L = L_{	ext{seg}} + \lambda L_{	ext{sra}}.
\]

- \(L_{	ext{seg}}\): original Mask2Former / FC-CLIP segmentation losses (`loss_ce`, `loss_mask`, `loss_dice`, etc.).
- \(L_{	ext{sra}}\): synthetic–real alignment loss defined as:

  \[
  L_{	ext{sra}} = 1 - \cos(F_s^p, M_r^p),
  \]

  where:
  - \(F_s^p\): feature of class \(p\) from **synthetic** samples (pooled CLIP dense feature with GT mask),
  - \(M_r^p\): **real** prototype of class \(p\), computed from an online memory bank of real features.

### 3.1 Real vs. Synthetic Branches

In the modified `FCCLIP.forward`:

- For **real** images (`is_synthetic = False`):
  - GT masks are resized to CLIP feature resolution.
  - Masked average pooling over `clip_vis_dense` yields per-object features \(F_r^p\).
  - These features are **only used to update** an online memory bank per class (queue length = `MEMORY_BANK_SIZE`).

- For **synthetic** images (`is_synthetic = True`):
  - The same masked pooling yields synthetic features \(F_s^p\).
  - For each object of class \(p\), we fetch the real prototype \(M_r^p\) from the memory bank (if available) and compute:

    \[
    L_{	ext{sra}}^{(p)} = 1 - \cos\left(F_s^p, M_r^p
ight).
    \]

  - `L_sra` is the average over all synthetic objects in the batch.

The total loss is:

```python
losses = L_seg_dict      # original SetCriterion losses
losses["loss_sra"] = lambda_sra * L_sra
```

where `lambda_sra` is a scalar hyperparameter.

### 3.2 Config: DreamMask Block

To enable the DreamMask alignment, add the following block into your FC-CLIP config YAML:

```yaml
MODEL:
  DREAMMASK:
    MEMORY_BANK_SIZE: 32   # β: queue length per class for real features
    LAMBDA_SRA: 0.1        # λ: weight on L_sra
```

and make sure your `MODEL.META_ARCHITECTURE` is set to `FCCLIP` using the modified `fcclip.py` that includes:

- `real_memory_bank`, `real_memory_ptr`
- `update_real_memory_bank`, `get_real_prototype`
- `compute_sra_loss(...)` in `forward(training)`.

---

## 4. How to Launch Training on the NSS Synthetic Dataset

Assume your FC-CLIP repo structure now looks like:

```text
train_net.py
train_net_nss.py          # <--- new script (NSS + DreamMask)
fcclip/
  meta_arch/
    fcclip.py             # <--- modified with SRA loss
  data/
    dataset_mappers/
      coco_instance_new_baseline_dataset_mapper.py   # <--- keeps is_synthetic
      coco_panoptic_new_baseline_dataset_mapper.py   # <--- keeps is_synthetic
configs/
  coco/
    panoptic-segmentation/
      fcclip/
        fcclip_convnext_large.yaml
processed_dataset/
  images/
  masks/
  scene_*.json
```

You can launch training with, for example:

```bash
python train_net_nss.py   --config-file configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large.yaml   --num-gpus 8   OUTPUT_DIR output/nss_fcclip_convnext_large
```

Notes:

- The config file still controls **backbone**, **learning rate**, **loss weights** for segmentation, etc.
- The *dataset* used for training is **forced** to be `("nss_processed_train",)` by `train_net_nss.py`.
- By default `DATASETS.TEST = ()`, so no evaluation is run during training.  
  If you later create a synthetic validation split (e.g., `nss_processed_val`), you can:
  - Register it similarly, and  
  - Modify `cfg.DATASETS.TEST` in `setup(args)` to include it.

---

## 5. Typical Training Recipe

1. **Prepare synthetic data** (in your NSS repo):

   ```bash
   # In your DreamMask-style NSS repo
   python CNA.py
   python batch_context_layout_generator.py
   # Layout-to-image with LayoutGPT (external)
   python mask_and_selection.py
   ```

2. **Copy or symlink `processed_dataset/` into FC-CLIP repo**:

   ```bash
   ln -s /path/to/processed_dataset /path/to/FC-CLIP/processed_dataset
   ```

3. **Place `train_net_nss.py` into the FC-CLIP root**, and make sure:

   - `fcclip/meta_arch/fcclip.py` is the DreamMask version (with SRA loss).
   - `COCOInstanceNewBaselineDatasetMapper` and `COCOPanopticNewBaselineDatasetMapper` propagate `is_synthetic`.

4. **Train**:

   ```bash
   cd IAT/fcclip

   python train_net_nss.py      --config-file configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large.yaml      --num-gpus 8      OUTPUT_DIR output/nss_fcclip_convnext_large
   ```

---

## 6. Remarks

- The category space for the NSS dataset is **derived directly** from the `name` fields in the processed JSONs.  
  This means you can flexibly mix:
  - Original COCO categories, and
  - Novel categories generated by CNA and used in the layouts.

- The model is still an FC-CLIP model:
  - Class-agnostic segmenter,
  - In-vocabulary classifier,
  - Open-vocabulary (text) classifier, all built on a single frozen convolutional CLIP.

- The DreamMask-style extension adds:
  - An online real feature memory bank per class,
  - Synthetic–real alignment loss \(L_{	ext{sra}}\), using your synthetic layouts and real data to enforce feature consistency.

This setup lets you study how **synthetic, layout-driven NSS data + SRA** affects:
- In-vocabulary performance,
- Generalization to truly novel classes.
